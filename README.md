# Исследование модификаций сети Faster R-CNN для задачи поиска дефектов на поверхности металлопроката

## Цель работы
Целью данной работы является исследование и сравнение модификаций базовой архитектуры сети Faster R-CNN для повышения достоверности контроля поверхности металлопроката

## Модификация 1


## Модификация 2
Ссылка на статью: https://www.nature.com/articles/s41598-025-12740-x

Авторами предложено два ключевых изменения в базовую архитектуру Faster R-CNN для улучшения способности модели к извлечению тонких признаков и фокусировки на областях дефектов.

### Оптимизированный модуль слияния признаков (Feature Fusion Module - FFM)

Для улучшения интеграции семантических признаков высокого уровня и пространственных признаков низкого уровня в сеть пирамиды признаков (FPN) был внедрен специализированный модуль слияния. Модуль располагается между уровнями P3 и P2.
Входными данными модуля являются карты признаков высокого и низкого уровней, описываемые формулами:

$$
F_{high} \in \mathbb{R}^{C_{high} \times H_{high} \times W_{high}} \qquad (1)
$$

$$
F_{low} \in \mathbb{R}^{C_{low} \times H_{low} \times W_{low}} \qquad (2)
$$

где C, H и W обозначают количество каналов, высоту и ширину карты признаков соответственно.

Процесс формирования новой объединенной карты признаков Fnew описывается следующим выражением:

$$
F_{new} = \text{ReLU}[\text{BatchNorm}(F'_{high} + F'_{low})] \qquad (3)
$$

Пояснение к формулам:
- Fhigh и Flow — промежуточные представления карт признаков уровней P3 и P2 после операций повышения размерности (upsampling) и свертки.
- Для обеспечения пространственного согласования высокоуровневые признаки увеличиваются с помощью билинейной интерполяции до размеров низкоуровневых признаков.
- Сверточный слой с ядром 3×3 используется для стандартизации количества каналов обоих уровней до 256.
- Операция поэлементного суммирования (+) объединяет семантику и пространственную детализацию.
- BatchNorm(пакетная нормализация) стабилизирует распределение активаций, а ReLU (функция активации) вводит нелинейность и отсеивает шумовые значения.

### Легкий механизм канального внимания (Lightweight Channel Attention Mechanism — LCAM)

Модуль легкого канального внимания (LCAM) интегрирован в архитектуру улучшенного Faster R-CNN между выходом сети пирамиды признаков (FPN) и входом сети предложений регионов (RPN). Основная задача модуля — усилить фокус модели на значимых областях дефектов поверхности стали и подавить отклик на нерелевантные признаки фона. Это позволяет повысить дискриминационную способность модели, улучшая точность распознавания особенно мелких и сложных дефектов.

Структура модуля: Архитектура LCAM состоит из следующих последовательных слоев:
1.	Сверточный слой понижения размерности: Свертка с ядром 3×3, уменьшающая количество каналов входной карты признаков с 256 до 128.
2.	Слой активации ReLU: Вводит нелинейность, обнуляя отрицательные значения.
3.	Сверточный слой восстановления размерности: Свертка с ядром 3×3, возвращающая количество каналов с 128 обратно до 256.
4.	Слой активации Sigmoid: Генерирует веса внимания в диапазоне [0,1] для каждого канала.
5.	Операция слияния: Поэлементное умножение исходной карты признаков FPN на полученную карту весов внимания.

$$
\text{ReLU}(x) = \max(0, x) \qquad (4)
$$

$$
\sigma(x) = \frac{1}{1 + e^{-x}} \qquad (5)
$$

$$
W_1 \in \mathbb{R}^{C_{reduced} \times C \times 3 \times 3}, \quad W_2 \in \mathbb{R}^{C \times C_{reduced} \times 3 \times 3} \qquad (6)
$$

$$
b_1 \in \mathbb{R}^{1 \times C_{reduced}}, \quad b_2 \in \mathbb{R}^{2 \times C_{reduced}} \qquad (7)
$$

$$
F(X) = X \odot \sigma(W_2 * \max(0, W_Z * X + b_1) + b_2) \qquad (8)
$$

Пояснение к формулам:
- ReLU(x) и σ(x) (Формулы 4 и 5): Описывают нелинейные преобразования. ReLU сохраняет положительные активации после первого сверточного слоя, способствуя разреженности представлений. Сигмоидная функция σ(x) на выходе второго слоя сжимает значения в диапазон [0,1] интерпретируемые как веса важности каналов (1 — высокая важность, 0 — шум/фон).
- W1,W2 и b1,b2 (Формулы 6 и 7): Определяют обучаемые параметры двух сверточных операций.
- X — входная карта признаков (C=256).
- W1— веса первого слоя (понижение размерности до Creduced=128).
- W2— веса второго слоя (восстановление размерности до C=256).
- Ядро свертки имеет размер 3×3для сохранения пространственной структуры.
- F(X) (Формула 8): Описывает полный проход данных через модуль внимания.
- Wz∗X+b1: Первая свертка (в тексте статьи обозначена как Wz, что соответствует W1из Формулы 6).
- max(0,…): Применение ReLU.
- W2∗(...)+b2: Вторая свертка.
- σ(...)Применение сигмоиды для получения карты весов.
- X⊙...: Поэлементное умножение исходной карты признаков X на карту весов внимания.
- F(X): Итоговая уточненная карта признаков, передаваемая на вход RPN.

## Модификация 3

## Модификация 4
Авторы:
1. Залялутдинова Карина, 312496
2. Корнюшенков Роман, 339600
3. Алексанр Медведев, № ИСУ
